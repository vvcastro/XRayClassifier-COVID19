{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T03: X-Ray Classifier\n",
    "---\n",
    "\n",
    "A lo largo de este archivo se describen distintas estrategias para hacer la clasificación. \n",
    "\n",
    "Cada estrategia debe contar con los siguientes elementos:\n",
    "\n",
    "    1. Caracteristicas extraídas.\n",
    "    2. Selección y transformación de características.\n",
    "    3. Clasificador utilizado.\n",
    "    \n",
    "---\n",
    "### - Se cargan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# data directory path\n",
    "DATADIR = 'data'\n",
    "\n",
    "# carga de datos de training\n",
    "x_training = np.load(os.path.join(DATADIR, 'training', 'dataset_features.npy'), allow_pickle=True)\n",
    "label_training = np.load(os.path.join(DATADIR, 'training', 'dataset_labels.npy'), allow_pickle=True)\n",
    "d_train = np.array([int(y[1:3]) for y in label_training])\n",
    "\n",
    "# carga de datos de testing\n",
    "x_testing = np.load(os.path.join(DATADIR, 'testing', 'dataset_features.npy'), allow_pickle=True)\n",
    "label_testing = np.load(os.path.join(DATADIR, 'testing', 'dataset_labels.npy'), allow_pickle=True)\n",
    "d_test = np.array([int(y[1:3]) for y in label_testing])\n",
    "\n",
    "# features_label\n",
    "flabels = np.load(os.path.join(DATADIR, 'flabels.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only 713 features...\n"
     ]
    }
   ],
   "source": [
    "to_use = ['int', 'hog']\n",
    "\n",
    "use_columns = []\n",
    "for label in flabels:  # nombre de la feature\n",
    "    for ftype in to_use:  # nombres de las features que queremos usar\n",
    "        if ftype in label:\n",
    "            use_columns.append(*np.where(flabels == label)[0])  # agregamos el indice de la feature\n",
    "\n",
    "print(f'Using only {len(use_columns)} features...')\n",
    "\n",
    "# selecionamos solo esas columnas\n",
    "X_train = x_training[:, use_columns]\n",
    "X_test = x_testing[:, use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Selecting Features:   0%|          | 0.00/25.0 [00:00<?, ? features/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned features: 688(5040 samples).\n",
      "Normalized features: 688(5040 samples).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selecting Features: 100%|██████████| 25.0/25.0 [00:16<00:00, 1.54 features/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 25(5040 samples).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pybalu.feature_selection import clean\n",
    "from pybalu.feature_transformation import normalize\n",
    "from pybalu.feature_selection import sfs\n",
    "\n",
    "\n",
    "FEATURES = 25\n",
    "\n",
    "# Training: Cleaning\n",
    "sclean = clean(X_train,show=False)\n",
    "X_train_clean = X_train[:,sclean]\n",
    "print('Cleaned features: '+ str(X_train_clean.shape[1]) + '('+str(X_train_clean.shape[0])+' samples).')\n",
    "\n",
    "# Training: Normalization\n",
    "X_train_norm, a, b = normalize(X_train_clean)\n",
    "print('Normalized features: '+str(X_train_norm.shape[1])+ '('+str(X_train_norm.shape[0])+' samples).')\n",
    "\n",
    "# Training: Feature selection\n",
    "ssfs = sfs(X_train_norm, d_train, n_features=FEATURES ,method=\"fisher\", show=True)\n",
    "X_train_sfs = X_train_norm[:,ssfs]\n",
    "print('Selected features: '+str(X_train_sfs.shape[1])+ '('+str(X_train_sfs.shape[0])+' samples).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean,Norm,SFS features: 25(1260 samples)\n"
     ]
    }
   ],
   "source": [
    "# Testing: Cleaning\n",
    "X_test_clean = X_test[:,sclean]\n",
    "\n",
    "# Testing: Normalization\n",
    "X_test_norm = X_test_clean * a + b\n",
    "\n",
    "# Testing: Feature selection\n",
    "X_test_sfs = X_test_norm[:,ssfs]\n",
    "print('Clean,Norm,SFS features: '+str(X_test_sfs.shape[1])+ '('+str(X_test_sfs.shape[0])+' samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[417   2   1]\n",
      " [ 10 383  27]\n",
      " [ 26  95 299]]\n",
      "0.8722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Classification on Testing dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_sfs, d_train)\n",
    "ds = knn.predict(X_test_sfs)\n",
    "acc = accuracy_score(d_test, ds)\n",
    "cmatrix = confusion_matrix(d_test, ds)\n",
    "\n",
    "print(cmatrix)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        pca = PCA(n_components=commands[-1]['n_components'])\n",
    "        pca.fit(x_training)\n",
    "        print('Features reducidas a: ', commands[-1]['n_components'], ' componentes...')\n",
    "        x_training, x_testing = pca.transform(x_training), pca.transform(x_testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patrones-t3",
   "language": "python",
   "name": "patrones-t3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
